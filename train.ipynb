{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "import sys\n",
    "sys.path.append('src/')\n",
    "from get_network import get_network\n",
    "from training_utils import train\n",
    "from get_data import get_data\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, Y_test, X_val, Y_val, X_train, Y_train = get_data()\n",
    "\n",
    "# number of samples in each set\n",
    "print(len(X_train), len(X_val), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of features and number of classes\n",
    "print(X_train.shape[1], np.unique(Y_train).shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add label noise to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# proportion of noisy labels\n",
    "p = 0.3\n",
    "\n",
    "# number of samples in the train set\n",
    "n_samples = len(X_train)\n",
    "\n",
    "# choose which samples will have noisy labels\n",
    "is_noisy = np.random.choice([0, 1], size=(n_samples,), p=[1.0 - p, p]).astype('bool')\n",
    "\n",
    "# number of samples with noisy labels\n",
    "n_noisy_samples = is_noisy.sum()\n",
    "\n",
    "all_classes = np.unique(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create noisy labels\n",
    "Y_train_noisy = np.zeros_like(Y_train)\n",
    "\n",
    "# random uniform noise\n",
    "Y_train_noisy[is_noisy] = np.random.choice(all_classes, size=n_noisy_samples)\n",
    "\n",
    "# original labels\n",
    "Y_train_noisy[~is_noisy] = Y_train[~is_noisy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# original distribution of classes\n",
    "np.unique(Y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# after some labels are flipped\n",
    "np.unique(Y_train_noisy, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, dtype='float32')\n",
    "ohe.fit(Y_train.reshape(-1, 1))\n",
    "\n",
    "Y_test = ohe.transform(Y_test.reshape(-1, 1))\n",
    "Y_val = ohe.transform(Y_val.reshape(-1, 1))\n",
    "Y_train = ohe.transform(Y_train.reshape(-1, 1))\n",
    "Y_train_noisy = ohe.transform(Y_train_noisy.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a usual neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_net():\n",
    "    model = Sequential([\n",
    "        #Dropout(0.1, input_shape=(54,)),\n",
    "        Dense(100, input_shape=(54,)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "\n",
    "        #Dropout(0.1),\n",
    "        Dense(100),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "\n",
    "        Dense(7),\n",
    "        Activation('softmax'),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model1 = simple_net()\n",
    "\n",
    "# train on original train dataset\n",
    "model1.fit(\n",
    "    X_train, Y_train, epochs=100, batch_size=128, \n",
    "    validation_data=(X_val, Y_val), verbose=0,\n",
    "    callbacks=[EarlyStopping('val_acc', patience=10, verbose=1)]\n",
    ");\n",
    "\n",
    "model1.evaluate(X_train, Y_train, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = simple_net()\n",
    "\n",
    "# train on train dataset where some labels are corrupted\n",
    "model2.fit(\n",
    "    X_train, Y_train_noisy, epochs=100, batch_size=128, \n",
    "    validation_data=(X_val, Y_val), verbose=0,\n",
    "    callbacks=[EarlyStopping('val_acc', patience=10, verbose=1)]\n",
    ")\n",
    "\n",
    "model2.evaluate(X_train, Y_train_noisy, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.evaluate(X_val, Y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.evaluate(X_val, Y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, linewidth=120, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_val_pred = model2.predict_proba(X_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = confusion_matrix(Y_val.argmax(1), Y_val_pred.argmax(1))\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = conf + 1\n",
    "conf_norm = conf.T/(conf.sum(1))\n",
    "conf_norm = conf_norm.T # p(j|i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ideas:\n",
    "# 1. overfit with big nn, then use smaller\n",
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with noise adaptation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of batches in the train set\n",
    "print(len(X_train)/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph, ops = get_network(\n",
    "    initial_bias=np.log(conf_norm).astype('float32'),\n",
    "    architecture=[54, 100, 100, 7], \n",
    "    dropout=[0.0, 0.0, 0.1], \n",
    "    optimizer=tf.train.AdamOptimizer(1e-3),\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "losses1, losses2 = train(\n",
    "    0, graph, ops, X_train, Y_train_noisy, X_val, Y_val, \n",
    "    batch_size=128, num_epochs1=60, num_epochs2=50, steps_per_epoch=130, \n",
    "    validation_steps=27, patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot([x[0] for x in losses1], label='train');\n",
    "plt.plot([x[1] for x in losses1], label='test');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot([x[0] for x in losses2], label='train');\n",
    "plt.plot([x[1] for x in losses2], label='test');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
